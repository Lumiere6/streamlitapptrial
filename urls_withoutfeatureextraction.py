# -*- coding: utf-8 -*-
"""URLs_WithoutFeatureExtraction

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_Jx9vKNuwLS63X3Hn6HA4SVC_iV0Zn89
"""

import pandas as pd
import numpy as np
from keras.models import Sequential
from keras.layers import LSTM, Dense, Embedding
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from keras.preprocessing.sequence import pad_sequences
from keras.preprocessing.text import Tokenizer

# prompt: moun
from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
df = pd.read_csv('/content/drive/MyDrive/data_bal - 20000.csv')
df.sample(n=25).head(25)

#import pandas as pd
#df = pd.read_csv('/content/data_bal - 20000.csv')
#df.sample(n=25).head(25)

#import matplotlib.pyplot as plt
#from wordcloud import WordCloud
#phish_url = " ".join(i for i in df_phish.URLs)
#wordcloud = WordCloud(width=1600, height=800,colormap='Paired').generate(phish_url)
#plt.figure( figsize=(12,14),facecolor='k')
#plt.imshow(wordcloud, interpolation='bilinear')
#plt.axis("off")
#plt.tight_layout(pad=0)
#plt.show()

#benign_url = " ".join(i for i in df_benign.URLs)
#wordcloud = WordCloud(width=1600, height=800,colormap='Paired').generate(benign_url)
#plt.figure( figsize=(12,14),facecolor='k')
#plt.imshow(wordcloud, interpolation='bilinear')
#plt.axis("off")
#plt.tight_layout(pad=0)
#plt.show()

import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Embedding, Dense, SpatialDropout1D

max_words =50
tokenizer = Tokenizer(num_words=max_words, split=' ')
tokenizer.fit_on_texts(df["URLs"].values)
X = tokenizer.texts_to_sequences(df["URLs"].values)
X = pad_sequences(X)

y=df["Labels"]

df.Labels.value_counts()

from sklearn.model_selection import train_test_split

import numpy as np
X_train, X_test, y_train, y_test = train_test_split(np.array(X), np.array(y), test_size=0.2, random_state=1)

embedding_dimension=25
lstm_out = 10

model = Sequential()
model.add(Embedding(max_words, embedding_dimension, input_length=X.shape[1]))
model.add(SpatialDropout1D(0.4))
model.add(LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2))
model.add(Dense(1, activation='sigmoid'))
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
print(model.summary())

batch_size = 100
epochs = 10

import datetime
from tensorflow.keras.callbacks import EarlyStopping

early_stopping = EarlyStopping(monitor='val_loss', patience=5)

model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test, y_test), callbacks=[early_stopping], verbose=1)

# Evaluation
score, acc = model.evaluate(X_test, y_test, batch_size=batch_size)
print("Test accuracy: %.2f%%" % (acc * 100))

from sklearn.metrics import accuracy_score, f1_score
prediction = model.predict(X_test)
y_pred = (prediction > 0.5)
print("Accuracy of the model : ", accuracy_score(y_pred, y_test))
print('F1-score: ', f1_score(y_pred, y_test))

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt
print('Confusion matrix:')
cm=confusion_matrix(y_test,y_pred,labels=[0,1])
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[0,1])
disp.plot()
plt.show()

def calculate_precision(true_positives, false_positives):
    if true_positives + false_positives == 0:
        return 0
    else:
        return true_positives / (true_positives + false_positives)

# Example values for true positives and false positives
true_positives = 1835
false_positives = 133

precision = calculate_precision(true_positives, false_positives)
print("Precision:", precision)

def calculate_tpr(true_positives, false_negatives):
    if true_positives + false_negatives == 0:
        return 0
    else:
        return true_positives / (true_positives + false_negatives)

def calculate_fpr(false_positives, true_negatives):
    if false_positives + true_negatives == 0:
        return 0
    else:
        return false_positives / (false_positives + true_negatives)

# Example values for true positives, false negatives, false positives, and true negatives
true_positives = 1835
false_negatives = 108
false_positives = 133
true_negatives = 1924

tpr = calculate_tpr(true_positives, false_negatives)
fpr = calculate_fpr(false_positives, true_negatives)

print("True Positive Rate (TPR):", tpr)
print("False Positive Rate (FPR):", fpr)

model.predict(X_test)

new_urls = "http://shadetreetechnology.com/V4/validation/a111aedc8ae390eabcfa130e041a10a4"
new_sequences = tokenizer.texts_to_sequences(new_urls)
# Make predictions
new_padded = pad_sequences(new_sequences, maxlen=max_words)
predictions_prob = model.predict(new_padded)

!pip install lime

from lime.lime_text import LimeTextExplainer
class_names=[0,1]
explainer= LimeTextExplainer(class_names=class_names)

def predict_proba(arr):
  list_tokenized_ex = tokenizer.texts_to_sequences(arr)
  Ex = pad_sequences(list_tokenized_ex, maxlen=max_words)
  pred=model.predict(Ex)
  returnable=[]
  for i in pred:
    temp=i[0]
    returnable.append(np.array([1-temp,temp]))
  return np.array(returnable)

explainer.explain_instance(new_urls,predict_proba).show_in_notebook(text=True)

# Make predictions
#new_urls = ["http://shadetreetechnology.com/V4/validation/a111aedc8ae390eabcfa130e041a10a4"]
#new_sequences = tokenizer.texts_to_sequences(new_urls)
# Make predictions
#new_padded = pad_sequences(new_sequences, maxlen=max_length)
#predictions_prob = model.predict(new_padded)
#predictions = (predictions_prob > 0.5).astype("int32")
#for url, prediction in zip(new_urls, predictions):
   # label = "Phishing" if prediction == 1 else "Legitimate"
    #print(f"{url}: {label}")